{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7eiDWcM_MC3H"
   },
   "source": [
    "# <font color='red'>Implement SGD Classifier with Logloss and L2 regularization Using SGD without using sklearn</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Fk5DSPCLxqT-"
   },
   "source": [
    "<font color='red'> Importing packages</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "42Et8BKIxnsp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "NpSk3WQBx7TQ"
   },
   "source": [
    "<font color='red'>Creating custom dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BsMp0oWzx6dv"
   },
   "outputs": [],
   "source": [
    "# please don't change random_state\n",
    "X, y = make_classification(n_samples=50000, n_features=15, n_informative=10, n_redundant=5,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)\n",
    "# make_classification is used to create custom dataset \n",
    "# Please check this link (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "L8W2fg1cyGdX",
    "outputId": "029d4c84-03b2-4143-a04c-34ff49c88890"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 15), (50000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "x99RWCgpqNHw"
   },
   "source": [
    "<font color='red'>Splitting data into train and test </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0Kh4dBfVyJMP"
   },
   "outputs": [],
   "source": [
    "#please don't change random state\n",
    "# you need not standardize the data as it is already standardized\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "0DR_YMBsyOci",
    "outputId": "732014d9-1731-4d3f-918f-a9f5255ee149"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37500, 15), (37500,), (12500, 15), (12500,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "BW4OHswfqjHR"
   },
   "source": [
    "# <font color='red' size=5>SGD classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "id": "3HpvTwDHyQQy",
    "outputId": "5729f08c-079a-4b17-bf51-f9aeb5abb13b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(eta0=0.0001, learning_rate=&#x27;constant&#x27;, loss=&#x27;log&#x27;,\n",
       "              random_state=15, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(eta0=0.0001, learning_rate=&#x27;constant&#x27;, loss=&#x27;log&#x27;,\n",
       "              random_state=15, verbose=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
       "              random_state=15, verbose=2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpha : float\n",
    "# Constant that multiplies the regularization term. \n",
    "\n",
    "# eta0 : double\n",
    "# The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules.\n",
    "\n",
    "clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "clf\n",
    "# Please check this documentation (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "id": "YYaVyQ2lyXcr",
    "outputId": "dc0bf840-b37e-4552-e513-84b64f6c64c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.77, NNZs: 15, Bias: -0.316653, T: 37500, Avg. loss: 0.455552\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.91, NNZs: 15, Bias: -0.472747, T: 75000, Avg. loss: 0.394686\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.98, NNZs: 15, Bias: -0.580082, T: 112500, Avg. loss: 0.385711\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.02, NNZs: 15, Bias: -0.658292, T: 150000, Avg. loss: 0.382083\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.04, NNZs: 15, Bias: -0.719528, T: 187500, Avg. loss: 0.380486\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.05, NNZs: 15, Bias: -0.763409, T: 225000, Avg. loss: 0.379578\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.06, NNZs: 15, Bias: -0.795106, T: 262500, Avg. loss: 0.379150\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.06, NNZs: 15, Bias: -0.819925, T: 300000, Avg. loss: 0.378856\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.07, NNZs: 15, Bias: -0.837805, T: 337500, Avg. loss: 0.378585\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.08, NNZs: 15, Bias: -0.853138, T: 375000, Avg. loss: 0.378630\n",
      "Total training time: 0.16 seconds.\n",
      "Convergence after 10 epochs took 0.16 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(eta0=0.0001, learning_rate=&#x27;constant&#x27;, loss=&#x27;log&#x27;,\n",
       "              random_state=15, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(eta0=0.0001, learning_rate=&#x27;constant&#x27;, loss=&#x27;log&#x27;,\n",
       "              random_state=15, verbose=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
       "              random_state=15, verbose=2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=X_train, y=y_train) # fitting our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "EAfkVI6GyaRO",
    "outputId": "bc88f920-6531-4106-9b4c-4dabb6d72b47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.42336692,  0.18547565, -0.14859036,  0.34144407, -0.2081867 ,\n",
       "          0.56016579, -0.45242483, -0.09408813,  0.2092732 ,  0.18084126,\n",
       "          0.19705191,  0.00421916, -0.0796037 ,  0.33852802,  0.02266721]]),\n",
       " (1, 15),\n",
       " array([-0.8531383]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_, clf.coef_.shape, clf.intercept_\n",
    "#clf.coef_ will return the weights\n",
    "#clf.coef_.shape will return the shape of weights\n",
    "#clf.intercept_ will return the intercept term"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_-CcGTKgsMrY"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## <font color='red' size=5> Implement Logistic Regression with L2 regularization Using SGD: without using sklearn </font>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZR_HgjgS_wKu"
   },
   "source": [
    "<font color='blue'>Initialize weights </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GecwYV9fsKZ9"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(row_vector):\n",
    "    ''' In this function, we will initialize our weights and bias'''\n",
    "    #initialize the weights as 1d array consisting of all zeros similar to the dimensions of row_vector\n",
    "    w = np.zeros_like(row_vector)\n",
    "    #you use zeros_like function to initialize zero, check this link https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html\n",
    "    #initialize bias to zero\n",
    "    b = 0\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "A7I6uWBRsKc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "b = 0\n"
     ]
    }
   ],
   "source": [
    "dim=X_train[0] \n",
    "w,b = initialize_weights(dim)\n",
    "print('w =',(w))\n",
    "print('b =',str(b))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4MI5SAjP9ofN"
   },
   "source": [
    "<font color='red'>Grader function - 1 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Pv1llH429wG5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim=X_train[0] \n",
    "w,b = initialize_weights(dim)\n",
    "def grader_weights(w,b):\n",
    "  assert((len(w)==len(dim)) and b==0 and np.sum(w)==0.0)\n",
    "  return True\n",
    "grader_weights(w,b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QN83oMWy_5rv"
   },
   "source": [
    "<font color='blue'>Compute sigmoid </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "qPv4NJuxABgs"
   },
   "source": [
    "$sigmoid(z)= 1/(1+exp(-z))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "nAfmQF47_Sd6"
   },
   "outputs": [],
   "source": [
    "from math import exp\n",
    "def sigmoid(z):\n",
    "    ''' In this function, we will return sigmoid of z'''\n",
    "    # compute sigmoid(z) and return\n",
    "    sig = 1/(1+exp(-z))\n",
    "    return sig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9YrGDwg3Ae4m"
   },
   "source": [
    "<font color='red'>Grader function - 2</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "P_JASp_NAfK_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_sigmoid(z):\n",
    "  val=sigmoid(z)\n",
    "  assert(val==0.8807970779778823)\n",
    "  return True\n",
    "grader_sigmoid(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "gS7JXbcrBOFF"
   },
   "source": [
    "<font color='blue'> Compute loss </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lfEiS22zBVYy"
   },
   "source": [
    "$log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VaFDgsp3sKi6"
   },
   "outputs": [],
   "source": [
    "from cmath import log10\n",
    "from math import log\n",
    "\n",
    "\n",
    "def logloss(y_true,y_pred):\n",
    "    # you have been given two arrays y_true and y_pred and you have to calculate the logloss\n",
    "    #while dealing with numpy arrays you can use vectorized operations for quicker calculations as compared to using loops\n",
    "    #https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/VectorizedOperations.html\n",
    "    #https://www.geeksforgeeks.org/vectorized-operations-in-numpy/\n",
    "    #write your code here\n",
    "    l11 = y_true\n",
    "    l12 = np.log10(y_pred)\n",
    "    l1 = l11*l12\n",
    "    l21 = 1-y_true\n",
    "    l22 = 1-y_pred\n",
    "    l23 = np.log10(l22)\n",
    "    l2 = l21*l23\n",
    "    l3 = l1 + l2\n",
    "    l4 = np.sum(l3)\n",
    "    loss = (-1/len(y_true))*l4\n",
    "    return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Zs1BTXVSClBt"
   },
   "source": [
    "<font color='red'>Grader function - 3 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LzttjvBFCuQ5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#round off the value to 8 values\n",
    "def grader_logloss(true,pred):\n",
    "  loss=logloss(true,pred)\n",
    "  assert(np.round(loss,6)==0.076449)\n",
    "  return True\n",
    "true=np.array([1,1,0,1,0])\n",
    "pred=np.array([0.9,0.8,0.1,0.8,0.2])\n",
    "grader_logloss(true,pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tQabIadLCBAB"
   },
   "source": [
    "<font color='blue'>Compute gradient w.r.to  'w' </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YTMxiYKaCQgd"
   },
   "source": [
    "$dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)}$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "NMVikyuFsKo5"
   },
   "outputs": [],
   "source": [
    "\n",
    "#make sure that the sigmoid function returns a scalar value, you can use dot function operation\n",
    "\n",
    "\n",
    "def gradient_dw(x,y,w,b,alpha,N):\n",
    "    '''In this function, we will compute the gardient w.r.to w '''\n",
    "    dw = x*(y-sigmoid(np.dot((np.transpose(w)),x) + b)) - (alpha/N)*w\n",
    "    return dw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RUFLNqL_GER9"
   },
   "source": [
    "<font color='red'>Grader function - 4 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "WI3xD8ctGEnJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_dw(x,y,w,b,alpha,N):\n",
    "  grad_dw=gradient_dw(x,y,w,b,alpha,N)\n",
    "  assert(np.round(np.sum(grad_dw),5)==4.75684)\n",
    "  return True\n",
    "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
    "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
    "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
    "grad_y=0\n",
    "grad_w=np.array([ 0.03364887,  0.03612727,  0.02786927,  0.08547455, -0.12870234,\n",
    "       -0.02555288,  0.11858013,  0.13305576,  0.07310204,  0.15149245,\n",
    "       -0.05708987, -0.064768  ,  0.18012332, -0.16880843, -0.27079877])\n",
    "grad_b=0.5\n",
    "alpha=0.0001\n",
    "N=len(X_train)\n",
    "grader_dw(grad_x,grad_y,grad_w,grad_b,alpha,N)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LE8g84_GI62n"
   },
   "source": [
    "<font color='blue'>Compute gradient w.r.to 'b' </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "fHvTYZzZJJ_N"
   },
   "source": [
    "$ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0nUf2ft4EZp8"
   },
   "outputs": [],
   "source": [
    "#sb should be a scalar value\n",
    "def gradient_db(x,y,w,b):\n",
    "     '''In this function, we will compute gradient w.r.to b '''\n",
    "     db = y - sigmoid(np.dot((np.transpose(w)),x) + b)\n",
    "     return db"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pbcBzufVG6qk"
   },
   "source": [
    "<font color='red'>Grader function - 5 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "TfFDKmscG5qZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_db(x,y,w,b):\n",
    "  grad_db=gradient_db(x,y,w,b)\n",
    "  assert(np.round(grad_db,4)==-0.3714)\n",
    "  return True\n",
    "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
    "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
    "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
    "grad_y=0.5\n",
    "grad_b=0.1\n",
    "grad_w=np.array([ 0.03364887,  0.03612727,  0.02786927,  0.08547455, -0.12870234,\n",
    "       -0.02555288,  0.11858013,  0.13305576,  0.07310204,  0.15149245,\n",
    "       -0.05708987, -0.064768  ,  0.18012332, -0.16880843, -0.27079877])\n",
    "alpha=0.0001\n",
    "N=len(X_train)\n",
    "grader_db(grad_x,grad_y,grad_w,grad_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.37140716318513056"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_db(grad_x,grad_y,grad_w,grad_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction function used to compute predicted_y given the dataset X\n",
    "def pred(w,b, X):\n",
    "    N = len(X)\n",
    "    predict = []\n",
    "    for i in range(N):\n",
    "        z=np.dot(w,X[i])+b\n",
    "        predict.append(sigmoid(z))\n",
    "    return np.array(predict)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TCK0jY_EOvyU"
   },
   "source": [
    "<font color='blue'> Implementing logistic regression</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "dmAdc5ejEZ25"
   },
   "outputs": [],
   "source": [
    "def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0):\n",
    "    ''' In this function, we will implement logistic regression'''\n",
    "    #Here eta0 is learning rate\n",
    "    #implement the code as follows\n",
    "    # initalize the weights (call the initialize_weights(X_train[0]) function)\n",
    "    # for every epoch\n",
    "        # for every data point(X_train,y_train)\n",
    "           #compute gradient w.r.to w (call the gradient_dw() function)\n",
    "           #compute gradient w.r.to b (call the gradient_db() function)\n",
    "           #update w, b\n",
    "        # predict the output of x_train [for all data points in X_train] using pred function with updated weights\n",
    "        #compute the loss between predicted and actual values (call the loss function)\n",
    "        # store all the train loss values in a list\n",
    "        # predict the output of x_test [for all data points in X_test] using pred function with updated weights\n",
    "        #compute the loss between predicted and actual values (call the loss function)\n",
    "        # store all the test loss values in a list\n",
    "        # you can also compare previous loss and current loss, if loss is not updating then stop the process \n",
    "        # you have to return w,b , train_loss and test loss\n",
    "        \n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    w,b = initialize_weights(X_train[0]) # Initialize the weights\n",
    "    #write your code to perform SGD\n",
    "    for i in range(epochs):\n",
    "        for x,y in zip(X_train, y_train):\n",
    "            dw = gradient_dw(x,y,w,b,alpha,len(X_train))\n",
    "            db = gradient_db(x,y,w,b)\n",
    "            \n",
    "            w = w + eta0*dw\n",
    "            b = b + eta0*db\n",
    "            \n",
    "        ytr_pred = pred(w,b,X_train)\n",
    "        loss_train = logloss(y_train,ytr_pred)\n",
    "        train_loss.append(loss_train)\n",
    "        yts_pred = pred(w,b, X_test)\n",
    "        loss_test = logloss(y_test,yts_pred)\n",
    "        test_loss.append(loss_test)\n",
    "        if i ==0:\n",
    "            continue\n",
    "        else:\n",
    "            #while abs(train_loss[i] - train_loss[i-1]) > 0.001:\n",
    "            if abs(train_loss[i] - train_loss[i-1]) > 0.001:    \n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    return w,b,train_loss,test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "sUquz7LFEZ6E"
   },
   "outputs": [],
   "source": [
    "alpha=0.001\n",
    "eta0=0.001\n",
    "N=len(X_train)\n",
    "epochs=20\n",
    "w,b,train_loss,test_loss=train(X_train,y_train,X_test,y_test,epochs,alpha,eta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.4135543   0.19231732 -0.14987872  0.32635477 -0.22458419  0.58614342\n",
      " -0.42722442 -0.10039941  0.21459638  0.15528446  0.17857722 -0.01297531\n",
      " -0.06477708  0.36312146 -0.00988464]\n",
      "-0.8990372632821858\n"
     ]
    }
   ],
   "source": [
    "#print thr value of weights w and bias b\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.00981262,  0.00684167, -0.00128836, -0.0150893 , -0.01639749,\n",
       "          0.02597763,  0.02520041, -0.00631128,  0.00532318, -0.0255568 ,\n",
       "         -0.01847469, -0.01719447,  0.01482662,  0.02459345, -0.03255185]]),\n",
       " array([-0.04589897]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the results we got after we implemented sgd and found the optimal weights and intercept\n",
    "\n",
    "w-clf.coef_, b-clf.intercept_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "l3eF_VSPSH2z"
   },
   "source": [
    "Compare your implementation and SGDClassifier's the weights and intercept, make sure they are as close as possible i.e difference should be in order of 10^-2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Grader function - 6 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The custom weights are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this grader function should return True\n",
    "#the difference between custom weights and clf.coef_ should be less than or equal to 0.05\n",
    "def differece_check_grader(w,b,coef,intercept):\n",
    "    val_array=np.abs(np.array(w-coef))\n",
    "    assert(np.all(val_array<=0.05))\n",
    "    print('The custom weights are correct')\n",
    "    return True\n",
    "differece_check_grader(w,b,clf.coef_,clf.intercept_)   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "230YbSgNSUrQ"
   },
   "source": [
    "<font color='blue'>Plot your train and test loss vs epochs </font>\n",
    "\n",
    "plot epoch number on X-axis and loss on Y-axis and make sure that the curve is converging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_num = []\n",
    "for i in range(len(train_loss)):\n",
    "    epoch_num.append(i+1)\n",
    "\n",
    "epoch_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "1O6GrRt7UeCJ"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.grid()\n",
    "plt.title('logistic regression log loss convergence curve')\n",
    "plt.scatter(epoch_num, train_loss, label = 'log loss train')\n",
    "plt.scatter(epoch_num, test_loss, label = 'log loss test')\n",
    "plt.plot(epoch_num, train_loss)\n",
    "plt.plot(epoch_num, test_loss)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-k28U1xDsLIO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMokBfs3-2PY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc07d24e2f18896857f0b2a651fe84ba40ce7b297e58d8804a308c8039f752a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
